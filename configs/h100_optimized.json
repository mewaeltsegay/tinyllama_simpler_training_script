{
  "model": {
    "checkpoint_path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "tokenizer_path": "tokenizer/",
    "vocab_size": 32000
  },
  "training": {
    "learning_rate": 2e-4,
    "batch_size": 32,
    "gradient_accumulation_steps": 4,
    "max_steps": 5000,
    "warmup_steps": 200,
    "save_steps": 250,
    "eval_steps": 100,
    "mixed_precision": "bf16",
    "gradient_checkpointing": true,
    "max_grad_norm": 1.0,
    "weight_decay": 0.01,
    "adam_beta1": 0.9,
    "adam_beta2": 0.95,
    "adam_epsilon": 1e-8
  },
  "data": {
    "tigrinya_dataset": "dataset/train.jsonl",
    "validation_dataset": "dataset/validation.jsonl",
    "english_validation": null,
    "max_length": 512,
    "debug_samples": null,
    "shuffle": true,
    "drop_last": true
  },
  "hardware": {
    "device": "auto",
    "num_gpus": 1,
    "dataloader_workers": 8,
    "pin_memory": true,
    "prefetch_factor": 4,
    "persistent_workers": true
  },
  "optimization": {
    "compile_model": false,
    "use_flash_attention": true,
    "memory_efficient_attention": true,
    "fused_optimizer": true,
    "channels_last": true
  },
  "knowledge_preservation": {
    "enabled": false,
    "english_weight": 0.0,
    "regularization_strength": 0.0,
    "validation_frequency": 100
  },
  "logging": {
    "log_level": "INFO",
    "wandb_project": "tigrinya-tinyllama-h100",
    "tensorboard_dir": "logs/h100_optimized",
    "save_metrics": true,
    "log_memory_usage": true
  }
}